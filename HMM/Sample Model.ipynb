{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piohmm import HMM\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows two examples of applications of PIOHMMs. \n",
    "\n",
    "### Create a synthetic dataset\n",
    "\n",
    "Data is generated in the following manner. First the sequence of latent states is generated following the distributions $$z_{i,1} \\sim \\textrm{Cat}(\\pi), \\quad z_{i,t}|z_{i,t-1}=j \\sim \\textrm{Cat}(A_j)$$ where $i$ is the sample index and $t$ is the time index. Observations are then generated $x_{i,t} | z_{i,t}=k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$. For each sample, a personalized offset is sampled $r_i \\sim \\textrm{Unif}[-b,b]$. Lastly structured noise is added to the observation $$\\hat{x}_i | x_i, r_i \\sim \\mathcal{N}(x_i+r_i, \\Sigma_T)$$ where $\\Sigma_T$ is specified via a squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f16e1015b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 #number of samples\n",
    "d = 1 #dimensionality of observations\n",
    "t = 20 #number of time steps\n",
    "k = 2 #number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[0.6, 0.4],[0.27, 0.73]]) #transition matrix\n",
    "a_dist = torch.distributions.dirichlet.Dirichlet(3 * torch.ones(k))\n",
    "pi = a_dist.sample() #inital state distributoin\n",
    "\n",
    "mu = torch.tensor([0., 2.]) # state means\n",
    "var = torch.tensor([0.1, 0.1]) #state covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2 #specify the range of a uniform distribution over personalized state effects, e.g. r_i ~ Unif[-b, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((n,t,d))\n",
    "Z = torch.zeros((n,t), dtype=torch.long)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(t):\n",
    "        if j == 0:\n",
    "            Z[i, j] = torch.multinomial(pi, num_samples=1).byte()\n",
    "            # D[i, j] = torch.rand(1)\n",
    "            m_dist = torch.distributions.normal.Normal(\n",
    "                mu.index_select(0, Z[i, j]),\n",
    "                var.index_select(0, Z[i, j]))\n",
    "            X[i, j, :] = m_dist.sample()\n",
    "        else:\n",
    "            Z[i, j] = torch.multinomial(A[Z[i, j - 1], :], num_samples=1)\n",
    "            # D[i, j] = torch.rand(1)\n",
    "            m_dist = torch.distributions.normal.Normal(\n",
    "                mu.index_select(0, Z[i, j]),\n",
    "                var.index_select(0, Z[i, j]))\n",
    "            X[i, j, :] = m_dist.sample()\n",
    "\n",
    "X_hat = torch.zeros(n, t, d)\n",
    "\n",
    "l = 2.5 #lengthscale for the SE kernel\n",
    "s = 4 #sigma^2 for the SE kernel\n",
    "\n",
    "#build covariance matrix\n",
    "var_x = torch.zeros(t,t)\n",
    "t_vec = torch.range(0,t)\n",
    "for j in range(t):\n",
    "    for jj in range(t):\n",
    "        r = (t_vec[j] - t_vec[jj])**2\n",
    "        var_x[j, jj] = 1/s*torch.exp(-r/(2*l))\n",
    "\n",
    "L = torch.cholesky(var_x)\n",
    "b_stor = torch.zeros(n)\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    e = torch.randn(t)\n",
    "    b_stor[i] = 2*b*torch.rand(1) - b\n",
    "    X_hat[i, :, :] =  torch.einsum('ik,k->i', [L, e])[None, :, None] + X[i, :, :] + b_stor[i]*torch.ones(1,t,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn model\n",
    "\n",
    "Using piohmm, two models are learned, one with personalized effects and one without (i.e. a standard HMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "masked_select: expected BoolTensor for mask",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#fit a personalized hmm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m piohmm \u001b[38;5;241m=\u001b[39m HMM(X_hat, k\u001b[38;5;241m=\u001b[39mk, full_cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, priorV\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, personalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, personalized_io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                    state_io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, UT\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-18\u001b[39m, priorMu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, var_fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m piohmm_params, _, _, elbo, b_hat, _  \u001b[38;5;241m=\u001b[39m \u001b[43mpiohmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m piohmm_mps \u001b[38;5;241m=\u001b[39m piohmm\u001b[38;5;241m.\u001b[39mpredict_sequence(piohmm_params, n_sample\u001b[38;5;241m=\u001b[39mb_hat)\n\u001b[0;32m      7\u001b[0m piohmm_xhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n,t))\n",
      "File \u001b[1;32m~\\Akash\\Disease Progression Modelling\\HMM\\piohmm.py:935\u001b[0m, in \u001b[0;36mHMM.learn_model\u001b[1;34m(self, num_iter, use_cc, cc, intermediate_save, load_model, model_name)\u001b[0m\n\u001b[0;32m    932\u001b[0m         prev_cost \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;66;03m#m-step, update the parameters\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperso_io \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperso:\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params, e_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mll, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melbo, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_hat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_hat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu_hat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_hat\n",
      "File \u001b[1;32m~\\Akash\\Disease Progression Modelling\\HMM\\piohmm.py:855\u001b[0m, in \u001b[0;36mHMM.m_step\u001b[1;34m(self, e_out, params, samples)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[1;32m--> 855\u001b[0m         logA[i, j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mom\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyte\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \\\n\u001b[0;32m    856\u001b[0m                           torch\u001b[38;5;241m.\u001b[39mlogsumexp(torch\u001b[38;5;241m.\u001b[39mmasked_select(xi[i, :, :, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mom[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mbyte()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    857\u001b[0m A \u001b[38;5;241m=\u001b[39m logA\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mut:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: masked_select: expected BoolTensor for mask"
     ]
    }
   ],
   "source": [
    "#fit a personalized hmm\n",
    "piohmm = HMM(X_hat, k=k, full_cov=False, priorV=False, io=False, personalized=True, personalized_io=False,\n",
    "                   state_io=False, UT=False, device='cpu', eps=1e-18, priorMu=True, var_fill=0.5, lr=0.005)\n",
    "piohmm_params, _, _, elbo, b_hat, _  = piohmm.learn_model(num_iter=1000, intermediate_save=False)\n",
    "piohmm_mps = piohmm.predict_sequence(piohmm_params, n_sample=b_hat)\n",
    "\n",
    "piohmm_xhat = np.zeros((n,t))\n",
    "piohmm_xvar = np.zeros((n,t))\n",
    "for i in range(n):\n",
    "    for j in range(t):\n",
    "        idx = np.where(piohmm_mps[i, j].numpy() == np.arange(k))[0][0]\n",
    "        piohmm_xhat[i,j] = piohmm_params['mu'][idx].numpy() + b_hat[i].detach().numpy()\n",
    "        piohmm_xvar[i,j] = 2*np.sqrt(piohmm_params['var'][idx].numpy())\n",
    "\n",
    "#fit a standard hmm\n",
    "hmm = HMM(X_hat, k=k, full_cov=False, priorV=False, io=False, personalized=False, personalized_io=False,\n",
    "             state_io=False, UT=False, device='cpu', eps=1e-18)\n",
    "hmm_params, _, ll = hmm.learn_model(num_iter=100, intermediate_save=False)\n",
    "hmm_mps = hmm.predict_sequence(hmm_params)\n",
    "\n",
    "hmm_xhat = np.zeros((n,t))\n",
    "hmm_xvar = np.zeros((n,t))\n",
    "for i in range(n):\n",
    "    for j in range(t):\n",
    "        idx = np.where(hmm_mps[i,j].numpy() == np.arange(k))[0][0]\n",
    "        hmm_xhat[i,j] = hmm_params['mu'][idx].numpy() \n",
    "        hmm_xvar[i,j] = 2*np.sqrt(hmm_params['var'][idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(12,5)\n",
    "axs[0].plot(elbo)\n",
    "axs[0].set_xlabel('Iterations')\n",
    "axs[0].set_ylabel('Objective Value')\n",
    "axs[1].plot(ll[:100])\n",
    "axs[1].set_xlabel('Iterations')\n",
    "axs[1].set_ylabel('Objective Value')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The below plot shows the 'observed data' in black and the inferred mean and variance from the personalized (blue) and standard (orange) HMMs. Because the classic HMM has no way of accounting for the personalized differences, the resulting model compensates by inflating the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,dpi=200)\n",
    "fig.set_size_inches(12,5)\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.plot(X_hat[i, :].numpy(), 'k:', label='$\\hat{x}_i$')\n",
    "    ax.plot(piohmm_xhat[i, :], label='Personalized $(\\mu_k + r_i) \\pm 2\\sigma_{k,i}$')\n",
    "    ax.fill_between(np.arange(t), piohmm_xhat[i,:] - piohmm_xvar[i, :], piohmm_xhat[i, :] + piohmm_xvar[i, :], alpha=0.5)\n",
    "    ax.plot(hmm_xhat[i, :], label='Standard $(\\mu_k + r_i) \\pm 2\\sigma_{k,i}$')\n",
    "    ax.fill_between(np.arange(t), hmm_xhat[i,:] - hmm_xvar[i, :], hmm_xhat[i, :] + hmm_xvar[i, :], alpha=0.5)\n",
    "    ax.set_title('Sample ' + str(i))\n",
    "fig.tight_layout()  \n",
    "ax.legend(loc='lower center', bbox_to_anchor=(-2.2, -0.95),\n",
    "          fancybox=True, shadow=True, ncol=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot shows the predicted personalized effects as compared to the inferred personalized effects. Overall, the model is successful in discovering these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(b_hat.detach().numpy(),b_stor)\n",
    "plt.plot(np.linspace(-b-0.5,b+0.5),np.linspace(-b-0.5,b+0.5),'k')\n",
    "plt.axis('square')\n",
    "plt.xlabel('Predicted r_i')\n",
    "plt.ylabel('True r_i')\n",
    "plt.xlim([-b-0.5,b+0.5])\n",
    "plt.ylim([-b-0.5,b+0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
